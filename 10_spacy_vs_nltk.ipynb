{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "de"
   },
   "source": [
    "## SpaCy-Modelle über das Notebook herunterladen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "Collecting en-core-web-sm==3.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0-py3-none-any.whl (13.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.6 MB 9.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from en-core-web-sm==3.1.0) (3.1.0)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.3.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.24.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.8.2)\n",
      "Requirement already satisfied: setuptools in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (49.2.0.post20200714)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.0.5)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.7 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (8.0.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.7.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (20.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.18.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (4.47.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.6.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.5)\n",
      "Requirement already satisfied: jinja2 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.11.2)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (7.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.25.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.10.0.0)\n",
      "Requirement already satisfied: six in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (5.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.1.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'de' are deprecated. Please use the\n",
      "full pipeline package name 'de_core_news_sm' instead.\u001b[0m\n",
      "Collecting de-core-news-sm==3.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.1.0/de_core_news_sm-3.1.0-py3-none-any.whl (18.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 18.8 MB 10.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from de-core-news-sm==3.1.0) (3.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (20.4)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.7 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (8.0.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (0.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (3.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (4.47.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (3.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (1.18.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (1.0.5)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (0.3.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (1.8.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (0.7.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (2.0.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (2.4.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (2.0.4)\n",
      "Requirement already satisfied: jinja2 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (2.11.2)\n",
      "Requirement already satisfied: setuptools in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (49.2.0.post20200714)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (2.24.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (0.6.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (2.4.7)\n",
      "Requirement already satisfied: six in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (1.15.0)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (7.1.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (3.10.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from jinja2->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (1.1.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (2020.6.20)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/ulizellbeck/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->de-core-news-sm==3.1.0) (5.1.0)\n",
      "Installing collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-3.1.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en # englisches Modell\n",
    "!python -m spacy download de # deutsches Modell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "de"
   },
   "source": [
    "## Import von SpaCy- und NLTK-Bibliotheken und Erstellung von \"Sprachverarbeitungs-Pipelines\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ulizellbeck/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/ulizellbeck/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/ulizellbeck/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_de = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "de"
   },
   "source": [
    "## Textumwandlung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "de"
   },
   "source": [
    "### Segmentierung von Text in Sätze\n",
    "\n",
    "Es geht darum, den Anfang und das Ende jedes Satzes eines Textes zu lokalisieren, um ihn segmentieren zu können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen des zu segmentierenden Textes\n",
    "text_de = \"UV sei nach dem Aussteigen aus ihrem PKW auf einen Stein getreten und habe seit dem Schmerzen im Bereich der rechten Fußsohle. Das hat ganz schön weh getan. Und jetzt kommt der dritte Satz.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "de"
   },
   "source": [
    "**Mit SpaCy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text durch die Pipeline leiten\n",
    "doc_de = nlp_de(text_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UV sei nach dem Aussteigen aus ihrem PKW auf einen Stein getreten und habe seit dem Schmerzen im Bereich der rechten Fußsohle.\n",
      "Das hat ganz schön weh getan.\n",
      "Und jetzt kommt der dritte Satz.\n"
     ]
    }
   ],
   "source": [
    "# Segmentierung des Textes und Anzeige des Ergebnisses\n",
    "for sent in doc_de.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mit NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import der Funktion, die die Segmentierung in Sätzen erlaubt\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UV sei nach dem Aussteigen aus ihrem PKW auf einen Stein getreten und habe seit dem Schmerzen im Bereich der rechten Fußsohle.\n",
      "Das hat ganz schön weh getan.\n",
      "Und jetzt kommt der dritte Satz.\n"
     ]
    }
   ],
   "source": [
    "# Segmentierung des Textes und Anzeige des Ergebnisses\n",
    "sentences = sent_tokenize(text_de, language = 'german')\n",
    "for sent in sentences:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "de"
   },
   "source": [
    "### Textsegmentierung in Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellung des zu tokenisierenden Satzes\n",
    "text_fr = \"UV sei nach dem Aussteigen aus ihrem PKW auf einen Stein getreten und habe seit dem Schmerzen im Bereich der rechten Fußsohle.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SpaCy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text durch die Pipeline leiten\n",
    "doc_de = nlp_de(text_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UV', 'sei', 'nach', 'dem', 'Aussteigen', 'aus', 'ihrem', 'PKW', 'auf', 'einen', 'Stein', 'getreten', 'und', 'habe', 'seit', 'dem', 'Schmerzen', 'im', 'Bereich', 'der', 'rechten', 'Fußsohle', '.', 'Das', 'hat', 'ganz', 'schön', 'weh', 'getan', '.', 'Und', 'jetzt', 'kommt', 'der', 'dritte', 'Satz', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenisierung des Satzes und Anzeige des Ergebnisses\n",
    "words = [w.text for w in doc_de]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mit NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importieren der Tokenisierungsfunktion\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UV', 'sei', 'nach', 'dem', 'Aussteigen', 'aus', 'ihrem', 'PKW', 'auf', 'einen', 'Stein', 'getreten', 'und', 'habe', 'seit', 'dem', 'Schmerzen', 'im', 'Bereich', 'der', 'rechten', 'Fußsohle', '.', 'Das', 'hat', 'ganz', 'schön', 'weh', 'getan', '.', 'Und', 'jetzt', 'kommt', 'der', 'dritte', 'Satz', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenisierung des Satzes und Anzeige des Ergebnisses\n",
    "words = word_tokenize(text_de, language = 'german')\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "de"
   },
   "source": [
    "## Informationsextraktion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "de"
   },
   "source": [
    "### Kennzeichnung von Wortarten\n",
    "\n",
    "Diese Aufgabe besteht darin, ein Wort seiner morphosyntaktischen Klasse (Substantiv, Verb, Adjektiv, ...) zuzuordnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellung von Testsätzen, einer auf Deutsch und einer auf Englisch\n",
    "text_de = \"Ich gehe mit meinem Hund in den Park\"\n",
    "text_en = \"I go to the park with my dog\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mit SpaCy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Durchlaufen der Sätze durch die jeweiligen Pipelines\n",
    "doc_de = nlp_de(text_de)\n",
    "doc_en = nlp_en(text_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word : Ich, , Tag : PPER\n",
      "Word : gehe, , Tag : VVFIN\n",
      "Word : mit, , Tag : APPR\n",
      "Word : meinem, , Tag : PPOSAT\n",
      "Word : Hund, , Tag : NN\n",
      "Word : in, , Tag : APPR\n",
      "Word : den, , Tag : ART\n",
      "Word : Park, , Tag : NN\n"
     ]
    }
   ],
   "source": [
    "# Anzeige jedes Tokens des deutschen Worts, gefolgt von seinem \"Tag\" \n",
    "for token in doc_de:\n",
    "    print('Word : {0}, , Tag : {1}' .format(token.text, token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word : I, , Tag : PRP\n",
      "Word : go, , Tag : VBP\n",
      "Word : to, , Tag : IN\n",
      "Word : the, , Tag : DT\n",
      "Word : park, , Tag : NN\n",
      "Word : with, , Tag : IN\n",
      "Word : my, , Tag : PRP$\n",
      "Word : dog, , Tag : NN\n"
     ]
    }
   ],
   "source": [
    "# Anzeige jedes Tokens des englischen Worts, gefolgt von seinem \"Tag\" \n",
    "for token in doc_en:\n",
    "    print('Word : {0}, , Tag : {1}' .format(token.text, token.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mit NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importieren der Tagging-funktion\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisierung des englischen Satzes\n",
    "tokens_en = word_tokenize(text_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('go', 'VBP'), ('to', 'TO'), ('the', 'DT'), ('park', 'NN'), ('with', 'IN'), ('my', 'PRP$'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Markierung von Token und Anzeige des Ergebnisses\n",
    "tags_en = pos_tag(tokens_en)\n",
    "print (tags_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "de"
   },
   "source": [
    "### Named Entity Recognition (NER)\n",
    "\n",
    "Diese besteht darin, die Wörter in einem Text zu erkennen, die kategorisierbaren Begriffen entsprechen (Namen von Personen, Orten, Organisationen, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispieltext\n",
    "text_en = \"Mark Elliot Zuckerberg (born May 14, 1984) is a co-founder of Facebook.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mit SpaCy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_en = nlp_en(text_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word : Mark, , Entity : PERSON\n",
      "Word : Elliot, , Entity : PERSON\n",
      "Word : Zuckerberg, , Entity : PERSON\n",
      "Word : (, , Entity : \n",
      "Word : born, , Entity : \n",
      "Word : May, , Entity : DATE\n",
      "Word : 14, , Entity : DATE\n",
      "Word : ,, , Entity : DATE\n",
      "Word : 1984, , Entity : DATE\n",
      "Word : ), , Entity : \n",
      "Word : is, , Entity : \n",
      "Word : a, , Entity : \n",
      "Word : co, , Entity : \n",
      "Word : -, , Entity : \n",
      "Word : founder, , Entity : \n",
      "Word : of, , Entity : \n",
      "Word : Facebook, , Entity : \n",
      "Word : ., , Entity : \n"
     ]
    }
   ],
   "source": [
    "for token in doc_en:\n",
    "    print('Word : {0}, , Entity : {1}' .format(token.text, token.ent_type_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mit NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_en = word_tokenize(text_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_en = pos_tag(tokens_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Mark/NNP)\n",
      "  (PERSON Elliot/NNP Zuckerberg/NNP)\n",
      "  (/(\n",
      "  born/VBN\n",
      "  May/NNP\n",
      "  14/CD\n",
      "  ,/,\n",
      "  1984/CD\n",
      "  )/)\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  co-founder/NN\n",
      "  of/IN\n",
      "  (GPE Facebook/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "ner_en = ne_chunk(tags_en)\n",
    "print (ner_en)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "de"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "fr",
   "targetLang": "de",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
